{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Invoke basic libraries and give input file path directory","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:46.774939Z","iopub.execute_input":"2022-07-04T17:49:46.775326Z","iopub.status.idle":"2022-07-04T17:49:46.794166Z","shell.execute_reply.started":"2022-07-04T17:49:46.775295Z","shell.execute_reply":"2022-07-04T17:49:46.793236Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"The Matplotlib package is invoked for visualisations and the sklearn library for regression steps to do train, testsplit and get model accuracy","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n#from os import path\n#os.chdir('/kaggle/input/significance measure of product search')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:46.943334Z","iopub.execute_input":"2022-07-04T17:49:46.943985Z","iopub.status.idle":"2022-07-04T17:49:46.950948Z","shell.execute_reply.started":"2022-07-04T17:49:46.943954Z","shell.execute_reply":"2022-07-04T17:49:46.949750Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Invoking nltk for lemmetisation which weren't useful hence used pattern library later in below codes","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer # Root / Base Words.\nimport re # Regex.\nimport string\nfrom sklearn.feature_extraction import text\nps = PorterStemmer()\nlm = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:47.103280Z","iopub.execute_input":"2022-07-04T17:49:47.103648Z","iopub.status.idle":"2022-07-04T17:49:47.110177Z","shell.execute_reply.started":"2022-07-04T17:49:47.103618Z","shell.execute_reply":"2022-07-04T17:49:47.108806Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"The Stopwords library is called for the stopwords removal","metadata":{}},{"cell_type":"code","source":"# conda install -c conda-forge wordcloud (one time install)\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:47.367940Z","iopub.execute_input":"2022-07-04T17:49:47.368322Z","iopub.status.idle":"2022-07-04T17:49:47.373675Z","shell.execute_reply.started":"2022-07-04T17:49:47.368292Z","shell.execute_reply":"2022-07-04T17:49:47.372501Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Call the spacy, Counter functions for NLP tasks, word counting tasks","metadata":{}},{"cell_type":"code","source":"# !python -m spacy download en_core_web_sm (One time)\nimport spacy\nfrom spacy import displacy \nfrom collections import Counter\nimport en_core_web_sm # en --> English pre-trained \n# sm --> Small Model || md --> Medium Model || lg --> Large Model\n# Loading large model in python:  --> gensim, tokenize & then lemmi.\nnlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:47.550415Z","iopub.execute_input":"2022-07-04T17:49:47.550847Z","iopub.status.idle":"2022-07-04T17:49:48.533563Z","shell.execute_reply.started":"2022-07-04T17:49:47.550813Z","shell.execute_reply":"2022-07-04T17:49:48.532251Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Invoke the datasets-train, attributes, product_descriptions, test in variables train, attr, desc, test respectively from the files uploaded in Input directory data ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/significance-measure-of-product-search/train.csv',encoding = 'ISO-8859-1')\nattr = pd.read_csv('../input/significance-measure-of-product-search/attributes.csv')\ndesc = pd.read_csv('../input/significance-measure-of-product-search/product_descriptions.csv', encoding='ISO-8859-1')\ntest = pd.read_csv('../input/significance-measure-of-product-search/test.csv', encoding = 'ISO-8859-1')\n#ISO 8859-1 encodes as \"Latin alphabet no. 1\", consisting of 191 characters from the Latin script.","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:48.536392Z","iopub.execute_input":"2022-07-04T17:49:48.536775Z","iopub.status.idle":"2022-07-04T17:49:51.785279Z","shell.execute_reply.started":"2022-07-04T17:49:48.536743Z","shell.execute_reply":"2022-07-04T17:49:51.784397Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"Find the dimensions of the respective datasets using shape function","metadata":{}},{"cell_type":"code","source":"print(train.shape)\nprint(attr.shape)\nprint(desc.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:51.786739Z","iopub.execute_input":"2022-07-04T17:49:51.787556Z","iopub.status.idle":"2022-07-04T17:49:51.793620Z","shell.execute_reply.started":"2022-07-04T17:49:51.787525Z","shell.execute_reply":"2022-07-04T17:49:51.792319Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Find null values of train dataset","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:51.797289Z","iopub.execute_input":"2022-07-04T17:49:51.798077Z","iopub.status.idle":"2022-07-04T17:49:51.823982Z","shell.execute_reply.started":"2022-07-04T17:49:51.798029Z","shell.execute_reply":"2022-07-04T17:49:51.822814Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Give the bin range for relevance plot, and the occurences in the respective slabs","metadata":{}},{"cell_type":"code","source":"s_bin = [0.99,1,1.5,2,2.5,3]\ngroup_bin = train.groupby(pd.cut(train['relevance'],s_bin))\ngroup_bin.size()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:51.825649Z","iopub.execute_input":"2022-07-04T17:49:51.825972Z","iopub.status.idle":"2022-07-04T17:49:51.847699Z","shell.execute_reply.started":"2022-07-04T17:49:51.825908Z","shell.execute_reply":"2022-07-04T17:49:51.846313Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Import seaborn package thereby to plot the relevance score distribution of train dataset","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.catplot(x='relevance',kind = 'count', data = train,height = 5, aspect = 2.5, order=[1,1.33,1.67,2,2.33,2.67,3])\nplt.title('Relevance Scores Spread in Train Data')\nplt.xlabel('Relevance Scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:51.849616Z","iopub.execute_input":"2022-07-04T17:49:51.850477Z","iopub.status.idle":"2022-07-04T17:49:52.272708Z","shell.execute_reply.started":"2022-07-04T17:49:51.850409Z","shell.execute_reply":"2022-07-04T17:49:52.271829Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Find the null values in product description dataset","metadata":{}},{"cell_type":"code","source":"desc.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:52.274189Z","iopub.execute_input":"2022-07-04T17:49:52.274520Z","iopub.status.idle":"2022-07-04T17:49:52.303751Z","shell.execute_reply.started":"2022-07-04T17:49:52.274490Z","shell.execute_reply":"2022-07-04T17:49:52.302279Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"Find the null values in attributes dataset","metadata":{}},{"cell_type":"code","source":"attr.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:52.306007Z","iopub.execute_input":"2022-07-04T17:49:52.306535Z","iopub.status.idle":"2022-07-04T17:49:52.693718Z","shell.execute_reply.started":"2022-07-04T17:49:52.306477Z","shell.execute_reply":"2022-07-04T17:49:52.692541Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Removing common null values in attributes dataset in all prod uid, name, values (155 values dropped)","metadata":{}},{"cell_type":"code","source":"# Removing common null values:\nattr.loc[attr['product_uid'].isna()].head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:52.695870Z","iopub.execute_input":"2022-07-04T17:49:52.696344Z","iopub.status.idle":"2022-07-04T17:49:52.714742Z","shell.execute_reply.started":"2022-07-04T17:49:52.696297Z","shell.execute_reply":"2022-07-04T17:49:52.713489Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Display of the remaining null values present in value column of attributes dataset","metadata":{}},{"cell_type":"code","source":"attr = attr.dropna(subset = ['product_uid'])\nattr.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:52.719762Z","iopub.execute_input":"2022-07-04T17:49:52.720099Z","iopub.status.idle":"2022-07-04T17:49:53.238931Z","shell.execute_reply.started":"2022-07-04T17:49:52.720069Z","shell.execute_reply":"2022-07-04T17:49:53.237577Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Drop the id columns from the train and test datasets as they represent just the serial number and not used in modelling.","metadata":{}},{"cell_type":"code","source":"# Dropping 'id':\ntrain = train.drop('id', axis = 1)\ntest = test.drop('id', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:53.240603Z","iopub.execute_input":"2022-07-04T17:49:53.240985Z","iopub.status.idle":"2022-07-04T17:49:53.258021Z","shell.execute_reply.started":"2022-07-04T17:49:53.240954Z","shell.execute_reply":"2022-07-04T17:49:53.256947Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"Merging the product descriptions with train and test dataset using left join with common product uid","metadata":{}},{"cell_type":"code","source":"## Merging product_desc. with train:\ntrain = pd.merge(train,desc, how = 'left', on = 'product_uid' )\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:53.260146Z","iopub.execute_input":"2022-07-04T17:49:53.260666Z","iopub.status.idle":"2022-07-04T17:49:53.327592Z","shell.execute_reply.started":"2022-07-04T17:49:53.260619Z","shell.execute_reply":"2022-07-04T17:49:53.326213Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"test = pd.merge(test,desc, how = 'left', on = 'product_uid')\nprint(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:53.329666Z","iopub.execute_input":"2022-07-04T17:49:53.330471Z","iopub.status.idle":"2022-07-04T17:49:53.408541Z","shell.execute_reply.started":"2022-07-04T17:49:53.330402Z","shell.execute_reply":"2022-07-04T17:49:53.407047Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"Find maximum occurence of specific types in name column of attributes dataset using value_counts() function","metadata":{}},{"cell_type":"code","source":"attr['name'].value_counts().head(7)\n\n# Out of 1,24,428 unique IDS,MFG Brand Name is repeated most times (70%)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:53.410358Z","iopub.execute_input":"2022-07-04T17:49:53.410849Z","iopub.status.idle":"2022-07-04T17:49:53.719867Z","shell.execute_reply.started":"2022-07-04T17:49:53.410815Z","shell.execute_reply":"2022-07-04T17:49:53.718656Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Display the MFG Brand name columns in the attributes dataset","metadata":{}},{"cell_type":"code","source":"attr.loc[(attr['name'] == 'MFG Brand Name')].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:53.721516Z","iopub.execute_input":"2022-07-04T17:49:53.721814Z","iopub.status.idle":"2022-07-04T17:49:53.990044Z","shell.execute_reply.started":"2022-07-04T17:49:53.721785Z","shell.execute_reply":"2022-07-04T17:49:53.988836Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Separates brand column from attributes and assign in brand_attr variable","metadata":{}},{"cell_type":"code","source":"attr['brand'] = attr.loc[(attr['name'] == 'MFG Brand Name'), 'value']\nbrand_attr = attr.drop(['name','value'], axis=1)\nbrand_attr.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:53.991602Z","iopub.execute_input":"2022-07-04T17:49:53.991951Z","iopub.status.idle":"2022-07-04T17:49:54.630736Z","shell.execute_reply.started":"2022-07-04T17:49:53.991920Z","shell.execute_reply":"2022-07-04T17:49:54.629393Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Drop the null values in brand_attr","metadata":{}},{"cell_type":"code","source":"## Dropping null values:\nbrand_attr = brand_attr.dropna(axis = 'rows')\nbrand_attr.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:54.632811Z","iopub.execute_input":"2022-07-04T17:49:54.633644Z","iopub.status.idle":"2022-07-04T17:49:54.771408Z","shell.execute_reply.started":"2022-07-04T17:49:54.633594Z","shell.execute_reply":"2022-07-04T17:49:54.770054Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Merge the brand_attr with train and test dataset through left join via common product_uid","metadata":{}},{"cell_type":"code","source":"## Merging brand with train:\ntrain = pd.merge(train,brand_attr,how='left',on='product_uid')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:54.775382Z","iopub.execute_input":"2022-07-04T17:49:54.775764Z","iopub.status.idle":"2022-07-04T17:49:54.857293Z","shell.execute_reply.started":"2022-07-04T17:49:54.775733Z","shell.execute_reply":"2022-07-04T17:49:54.855826Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"## Merging brand with test:\ntest = pd.merge(test,brand_attr,how='left',on='product_uid')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:54.858807Z","iopub.execute_input":"2022-07-04T17:49:54.859173Z","iopub.status.idle":"2022-07-04T17:49:54.967557Z","shell.execute_reply.started":"2022-07-04T17:49:54.859142Z","shell.execute_reply":"2022-07-04T17:49:54.966551Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"Drop the brand column in the attributes dataset as it is merged with train, test respectively","metadata":{}},{"cell_type":"code","source":"## Dropping brand:\nattr = attr.drop('brand', axis = 'columns')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:54.969171Z","iopub.execute_input":"2022-07-04T17:49:54.969958Z","iopub.status.idle":"2022-07-04T17:49:55.065123Z","shell.execute_reply.started":"2022-07-04T17:49:54.969900Z","shell.execute_reply":"2022-07-04T17:49:55.063064Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"Drop the MFG Brand name containing names column in attributes dataset, to merge effectively as already used.","metadata":{}},{"cell_type":"code","source":"# Dropping rows that contains 'MFG Brand Name':\nattr.drop(attr.query(\"name == 'MFG Brand Name'\").index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:55.067517Z","iopub.execute_input":"2022-07-04T17:49:55.068321Z","iopub.status.idle":"2022-07-04T17:49:55.540141Z","shell.execute_reply.started":"2022-07-04T17:49:55.068259Z","shell.execute_reply":"2022-07-04T17:49:55.538289Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"Display the modified attributes dataset after modifications as stated in above steps","metadata":{}},{"cell_type":"code","source":"attr.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:55.542766Z","iopub.execute_input":"2022-07-04T17:49:55.543285Z","iopub.status.idle":"2022-07-04T17:49:55.559305Z","shell.execute_reply.started":"2022-07-04T17:49:55.543237Z","shell.execute_reply":"2022-07-04T17:49:55.557765Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"Concatenate the name and value columns of attributes dataset into combined_attr new column","metadata":{}},{"cell_type":"code","source":"# Concatening 2 text data:\nattr['combined_attr'] = attr['name'].str.cat(attr['value'],sep=' ')\nattr.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:55.561231Z","iopub.execute_input":"2022-07-04T17:49:55.561732Z","iopub.status.idle":"2022-07-04T17:49:56.956921Z","shell.execute_reply.started":"2022-07-04T17:49:55.561685Z","shell.execute_reply":"2022-07-04T17:49:56.955336Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Print the dimensions and check presence of null values in attributes dataset","metadata":{}},{"cell_type":"code","source":"print(attr.shape)\nattr.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:56.958655Z","iopub.execute_input":"2022-07-04T17:49:56.958992Z","iopub.status.idle":"2022-07-04T17:49:57.508007Z","shell.execute_reply.started":"2022-07-04T17:49:56.958962Z","shell.execute_reply":"2022-07-04T17:49:57.506851Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Locate and compare the null values in the value column with other columns ","metadata":{}},{"cell_type":"code","source":"attr.loc[attr['value'].isna()]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:57.510169Z","iopub.execute_input":"2022-07-04T17:49:57.510520Z","iopub.status.idle":"2022-07-04T17:49:57.861601Z","shell.execute_reply.started":"2022-07-04T17:49:57.510459Z","shell.execute_reply":"2022-07-04T17:49:57.860314Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"Dropping the null values in attributes dataset as name has no value specified","metadata":{}},{"cell_type":"code","source":"# Dropping null values since attribute name has no value\nattr = attr.dropna()\nattr.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:57.862968Z","iopub.execute_input":"2022-07-04T17:49:57.863262Z","iopub.status.idle":"2022-07-04T17:49:58.572482Z","shell.execute_reply.started":"2022-07-04T17:49:57.863232Z","shell.execute_reply":"2022-07-04T17:49:58.571068Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"Dropping name, value columns in attributes dataset as we are using only combined_attr for analysis ","metadata":{}},{"cell_type":"code","source":"attr = attr.drop(['name','value'], axis='columns')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:58.574110Z","iopub.execute_input":"2022-07-04T17:49:58.574595Z","iopub.status.idle":"2022-07-04T17:49:58.656286Z","shell.execute_reply.started":"2022-07-04T17:49:58.574545Z","shell.execute_reply":"2022-07-04T17:49:58.654800Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"Print the top 10 rows of the modified attributes dataset","metadata":{}},{"cell_type":"code","source":"attr.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:58.666218Z","iopub.execute_input":"2022-07-04T17:49:58.666634Z","iopub.status.idle":"2022-07-04T17:49:58.680889Z","shell.execute_reply.started":"2022-07-04T17:49:58.666601Z","shell.execute_reply":"2022-07-04T17:49:58.679023Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"Group the attributes so that combined_attr are got with unique respective product uIDS and merged using join row wise","metadata":{}},{"cell_type":"code","source":"# Grouping all attributes based on unique IDs:\nattr = attr.groupby(['product_uid'])['combined_attr'].apply(' '.join).reset_index()\nattr.head()\n# Reset_index drops the index value thereby can perform grouping in same data!","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:49:58.682639Z","iopub.execute_input":"2022-07-04T17:49:58.683017Z","iopub.status.idle":"2022-07-04T17:50:01.073794Z","shell.execute_reply.started":"2022-07-04T17:49:58.682970Z","shell.execute_reply":"2022-07-04T17:50:01.072579Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"Merging combines attributes column with train and test dataset via common product uid","metadata":{}},{"cell_type":"code","source":"# Merging Combined_attributes with train:\ntrain_c = pd.merge(train,attr[['product_uid','combined_attr']],how='left',on='product_uid')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.075472Z","iopub.execute_input":"2022-07-04T17:50:01.075811Z","iopub.status.idle":"2022-07-04T17:50:01.163657Z","shell.execute_reply.started":"2022-07-04T17:50:01.075780Z","shell.execute_reply":"2022-07-04T17:50:01.162510Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Merging Combined_attributes with test:\ntest_c = pd.merge(test,attr[['product_uid','combined_attr']],how='left',on='product_uid')\ntest_c.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.165544Z","iopub.execute_input":"2022-07-04T17:50:01.166023Z","iopub.status.idle":"2022-07-04T17:50:01.289589Z","shell.execute_reply.started":"2022-07-04T17:50:01.165973Z","shell.execute_reply":"2022-07-04T17:50:01.288172Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Find null values in modified test dataset ","metadata":{}},{"cell_type":"code","source":"test_c.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.291202Z","iopub.execute_input":"2022-07-04T17:50:01.291692Z","iopub.status.idle":"2022-07-04T17:50:01.391202Z","shell.execute_reply.started":"2022-07-04T17:50:01.291654Z","shell.execute_reply":"2022-07-04T17:50:01.389882Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Print the dimensions of train dataset and the top 10 rows","metadata":{}},{"cell_type":"code","source":"print(train_c.shape)\ntrain_c.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.393203Z","iopub.execute_input":"2022-07-04T17:50:01.393725Z","iopub.status.idle":"2022-07-04T17:50:01.413152Z","shell.execute_reply.started":"2022-07-04T17:50:01.393677Z","shell.execute_reply":"2022-07-04T17:50:01.411782Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"Find the null values of train dataset through isnull()","metadata":{}},{"cell_type":"code","source":"train_c.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.415646Z","iopub.execute_input":"2022-07-04T17:50:01.416243Z","iopub.status.idle":"2022-07-04T17:50:01.467787Z","shell.execute_reply.started":"2022-07-04T17:50:01.416195Z","shell.execute_reply":"2022-07-04T17:50:01.466781Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"Check the presence of non null values in brand column and compare side by side with the value_counts()","metadata":{}},{"cell_type":"code","source":"# Checking non-null values of brand & comparing it with product_title:\nprint(train_c.loc[~train_c['brand'].isna(),['product_title','brand']].value_counts().head(35))","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.469266Z","iopub.execute_input":"2022-07-04T17:50:01.469640Z","iopub.status.idle":"2022-07-04T17:50:01.603293Z","shell.execute_reply.started":"2022-07-04T17:50:01.469609Z","shell.execute_reply":"2022-07-04T17:50:01.601661Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Finding is that Brand contains atleast 1 word to atmost 4 words from product title column","metadata":{}},{"cell_type":"code","source":"## It can be noted that brand contains atleast 1 word to atmost 4 words from product title:\nbrand_u = np.unique(train_c['brand'].dropna())\nprint(brand_u.shape)\nbrand_u","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.605526Z","iopub.execute_input":"2022-07-04T17:50:01.606373Z","iopub.status.idle":"2022-07-04T17:50:01.695656Z","shell.execute_reply.started":"2022-07-04T17:50:01.606310Z","shell.execute_reply":"2022-07-04T17:50:01.694381Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"Check unique brands available","metadata":{}},{"cell_type":"code","source":"## It can be noted that brand contains atleast 1 word to atmost 4 words from product title:\nbrandtest_u = np.unique(test_c['brand'].dropna())\nprint(brandtest_u.shape)\nbrandtest_u","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.697633Z","iopub.execute_input":"2022-07-04T17:50:01.698078Z","iopub.status.idle":"2022-07-04T17:50:01.913087Z","shell.execute_reply.started":"2022-07-04T17:50:01.698046Z","shell.execute_reply":"2022-07-04T17:50:01.911608Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"Functions to replace null values in brand by 1st four words occuring in product title column of both train and test","metadata":{}},{"cell_type":"code","source":"def word(n,title):                            # n --> No. of words & joining them with spaces in product_title\n    return ' '.join(title.split()[:n])        # Defining the merge in iteration!","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.915528Z","iopub.execute_input":"2022-07-04T17:50:01.915885Z","iopub.status.idle":"2022-07-04T17:50:01.922050Z","shell.execute_reply.started":"2022-07-04T17:50:01.915853Z","shell.execute_reply":"2022-07-04T17:50:01.920509Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"def fillna_brand(dummy_tr, brand_u):              # Defining temp. data (for filling null) & unique brands:\n    missing = dummy_tr[dummy_tr['brand'].isna()].copy()\n    \n    for i, row in missing.iterrows():         # Iterate each row in the DataFrame\n            title = row['product_title']      # title --> All Rows of the product_title\n            if word(4, title) in brand_u:\n                    missing.loc[i,'brand'] = word(4,title)\n            elif word(3, title) in brand_u:\n                    missing.loc[i,'brand']  = word(3,title)\n            elif word(2, title) in brand_u:\n                    missing.loc[i,'brand']  = word(2,title)\n            else:\n                    missing.loc[i,'brand']  = word(1,title)\n    dummy_tr.loc[missing.index,'brand'] = missing['brand']\n    return dummy_tr","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.923854Z","iopub.execute_input":"2022-07-04T17:50:01.924183Z","iopub.status.idle":"2022-07-04T17:50:01.934664Z","shell.execute_reply.started":"2022-07-04T17:50:01.924152Z","shell.execute_reply":"2022-07-04T17:50:01.933288Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"def fillna_brand(dummy_tr, brandtest_u):              # Defining temp. data (for filling null) & unique brands:\n    missing = dummy_tr[dummy_tr['brand'].isna()].copy()\n    \n    for i, row in missing.iterrows():         # Iterate each row in the DataFrame\n            title = row['product_title']      # title --> All Rows of the product_title\n            if word(4, title) in brandtest_u:\n                    missing.loc[i,'brand'] = word(4,title)\n            elif word(3, title) in brandtest_u:\n                    missing.loc[i,'brand']  = word(3,title)\n            elif word(2, title) in brandtest_u:\n                    missing.loc[i,'brand']  = word(2,title)\n            else:\n                    missing.loc[i,'brand']  = word(1,title)\n    dummy_tr.loc[missing.index,'brand'] = missing['brand']\n    return dummy_tr","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.936804Z","iopub.execute_input":"2022-07-04T17:50:01.937269Z","iopub.status.idle":"2022-07-04T17:50:01.951288Z","shell.execute_reply.started":"2022-07-04T17:50:01.937232Z","shell.execute_reply":"2022-07-04T17:50:01.950297Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"train_c = fillna_brand(train_c,brand_u)\ntrain_c.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:01.952700Z","iopub.execute_input":"2022-07-04T17:50:01.953013Z","iopub.status.idle":"2022-07-04T17:50:22.871519Z","shell.execute_reply.started":"2022-07-04T17:50:01.952983Z","shell.execute_reply":"2022-07-04T17:50:22.870006Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"test_c = fillna_brand(test_c,brandtest_u)\ntest_c.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:50:22.873726Z","iopub.execute_input":"2022-07-04T17:50:22.874369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_c['brand'].value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Intrepretation non-null values in attributes: (Similar to description)\ntrain_c.loc[(~train_c['combined_attr'].isna(),['combined_attr','product_description'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replace null values in combined attr with prod_desc since both are similar","metadata":{}},{"cell_type":"code","source":"# Viewing null values for verification:\ntrain_c.loc[(train_c['combined_attr'].isna(),['combined_attr','product_description'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling null values in combined_attributes:\ntrain_c['combined_attr'] = train_c['combined_attr'].fillna(train_c.loc[train_c['combined_attr'].isna(),'product_description'])\ntrain_c.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling null values in combined_attributes of TEST:\ntest_c['combined_attr'] = test_c['combined_attr'].fillna(test_c.loc[test_c['combined_attr'].isna(),'product_description'])\ntest_c.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_c.iloc[302])\nprint(train_c.iloc[303]) # Checking the consolidated file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_c.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concatenating train and test_data into a single dataset named features","metadata":{}},{"cell_type":"code","source":"#Concatenating train_)data and test_data into dtaframe features\nfeatures = pd.concat((train_c, test_c), axis=0, ignore_index=True)\nprint(str(len(train_c)) + ' ' + str(len(test_c)) + ' ' + str(len(features)))\n#del train_le\n#del test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TEXT MINING steps (string_edit function applied) - \n1)Text treatment (lower)\n2)Units standardisation\n3)Special characters removal","metadata":{}},{"cell_type":"code","source":"def string_edit(s:str): \n    if isinstance(s, str):\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) \n        s = s.lower()\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\",\",\"\") \n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"//\",\"/\")\n        s = s.replace(\"..\",\".\")\n        #s = s.replace(\" / \",\" \") #it will convert fractional nu. to whole nu.\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        #s = re.sub(r\"(^\\.|/)\", r\"\", s)\n        s = re.sub(r\"(\\.|/)$\", r\"\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        #s = s.replace(\" x \",\" xbi \")\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" x \")\n        s = s.replace(\" by \",\" x \") #search term (100047)\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        \n        # Consolidate variations of equivalent unit terms \n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = s.replace(\"°\",\" degrees \")\n        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n        s = s.replace(\" v \",\" volts \")\n        s = re.sub(r\"([0-9]+)( *)(volt\\|volt)\\.?\", r\"\\1volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\" . \",\" \")\n        \n        # Handling numeric instances with common identifiers\n        s = re.sub(r\"zero\\.?\", r\"0 \", s)\n        s = re.sub(r\"one\\.?\", r\"1 \", s)\n        s = re.sub(r\"two\\.?\", r\"2 \", s)\n        s = re.sub(r\"three\\.?\", r\"3 \", s)\n        s = re.sub(r\"four\\.?\", r\"4 \", s)\n        s = re.sub(r\"five\\.?\", r\"5 \", s)\n        s = re.sub(r\"six\\.?\", r\"6 \", s)\n        s = re.sub(r\"seven\\.?\", r\"7 \", s)\n        s = re.sub(r\"eight\\.?\", r\"8 \", s)\n        s = re.sub(r\"nine\\.?\", r\"9 \", s)\n        \n        return s\n    else:\n        # Return a \"null\" string if the parameter supplied is not a string\n        return \"null\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features['search_term'] = features['search_term'].apply(str).apply(string_edit)\nfeatures['product_title'] = features['product_title'].apply(str).apply(string_edit)\nfeatures['combined_attr'] = features['combined_attr'].apply(str).apply(string_edit)\nfeatures['brand'] = features['brand'].apply(str).apply(string_edit)\nfeatures['product_description'] = features['product_description'].apply(str).apply(string_edit)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. HTML Tag removal (BeautifulSoup, lxml)","metadata":{}},{"cell_type":"code","source":"print('Total {} html tags contains in product description'.format(features.product_description.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in combined_attr'.format(features.combined_attr.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in product_title'.format(features.product_title.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in brand'.format(features.brand.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in search_term'.format(features.search_term.str.count('<br$').values.sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef remove_tags(raw_html):\n    return BeautifulSoup(raw_html, \"lxml\").text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features['product_description'] = features['product_description'].apply(remove_tags)\nfeatures['combined_attr'] = features['combined_attr'].apply(remove_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total {} html tags contains in product description'.format(features.product_description.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in combined_attr'.format(features.combined_attr.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in product_title'.format(features.product_title.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in brand'.format(features.brand.str.count('<br$').values.sum()))\nprint('Total {} html tags contains in search_term'.format(features.search_term.str.count('<br$').values.sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Stopwords removal by spacy nlp small model","metadata":{}},{"cell_type":"code","source":"stopwords = nlp.Defaults.stop_words\nprint(len(stopwords))\nstop = set(stopwords)\n#stopwords.remove('not')\n#stopwords.update('not only')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. Lemmetisation using pattern library","metadata":{}},{"cell_type":"code","source":"#!conda install -c asmeurer pattern -y\nimport pattern\nfrom pattern.en import lemma","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(x):\n    result = x.lower()\n    result = result.split()\n    result = \" \".join([lemma(word) for word in result\n                     if word not in stop])\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features['product_title'] = features['product_title'].apply(clean)\nfeatures['search_term'] = features['search_term'].apply(clean)\nfeatures['brand'] = features['brand'].apply(clean)\nfeatures['product_description'] = features['product_description'].apply(clean)\nfeatures['combined_attr'] = features['combined_attr'].apply(clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FEATURE ENGINEERING (part of text mining, NLP)","metadata":{}},{"cell_type":"markdown","source":"Import the libraries needed for feature engineering","metadata":{}},{"cell_type":"code","source":"import pickle\nimport keras as kr\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.layers import *\nimport spacy\nimport re\nimport math \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find null values in features final dataset just to ensure that null values in only relevance column","metadata":{}},{"cell_type":"code","source":"for col in features.columns:\n  print('{} - {} null values'.format(col, features[col].isna().values.sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Install textdistance to find the distance measures of jaccard, levenshtein distances","metadata":{}},{"cell_type":"code","source":"pip install textdistance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find the numerical values of jaccard, levenshtein distances for all the independent variable columns in features dataset\nJACCARD - a measure of dissimilarity\nLEVENSHTEIN - the minimum number of single-character edits required to change one word into the other.","metadata":{}},{"cell_type":"code","source":"import textdistance\n#with description\nfeatures[\"jaccard_similar_desc\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_desc\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\n#with title\nfeatures[\"jaccard_similar_title\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_title\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\n#with brand\nfeatures[\"jaccard_similar_brand\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_brand\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\n#with combined attr\nfeatures[\"jaccard_similar_CA\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"combined_attr\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_CA\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"combined_attr\"][i]) for i in range(0, len(features))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.to_csv(r'/kaggle/working/features.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Text embedding process - with embed similarity between the search term and other IV columns to find items similar to a given item. ","metadata":{}},{"cell_type":"code","source":"import xgboost\nimport gensim\nfrom time import time\nfrom gensim.models import KeyedVectors\nfrom gensim.utils import simple_preprocess, tokenize\nfrom nltk.corpus import brown\nembed_model = gensim.models.Word2Vec(brown.sents())\nembed_model.save('brown.embedding')\nmodel = gensim.models.Word2Vec.load('brown.embedding')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef embeding_similarity_calculator(s, t, i):\n    _sum = 0\n    avg = 0\n    if len(s.split()) == 0 :\n        return 0\n    for s_word in s.split():\n        _max = 0\n        for t_word in t.split():\n            if ((s_word in model.wv) and (t_word in model.wv)):\n                _max = max(_max, model.wv.similarity(s_word, t_word))\n        _sum += _max\n    avg = _sum/ len(s.split())\n    return avg\nfeatures[\"simdesc\"] = [embeding_similarity_calculator(features[\"search_term\"][i], features[\"product_description\"][i],i) for i in range(0, len(features))]\nfeatures[\"simtitle\"] = [embeding_similarity_calculator(features[\"search_term\"][i], features[\"product_title\"][i],i) for i in range(0, len(features))]\nfeatures[\"simbrand\"] = [embeding_similarity_calculator(features[\"search_term\"][i], features[\"brand\"][i],i) for i in range(0, len(features))]\nfeatures[\"simcomb\"] = [embeding_similarity_calculator(features[\"search_term\"][i], features[\"combined_attr\"][i],i) for i in range(0, len(features))]\nfeatures.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the library for TF-IDF step for cosine similarity - a metric that measures the cosine of the angle between two vectors projected in a multi-dimensional space.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(analyzer='char_wb', ngram_range = (3,3), max_features = 1500)\ntfidf_des = tfidf_vect.fit_transform(features.product_description).toarray()\ntfidf_brand = tfidf_vect.fit_transform(features.brand).toarray()\ntfidf_CA = tfidf_vect.fit_transform(features.combined_attr).toarray()\ntfidf_title = tfidf_vect.fit_transform(features.product_title).toarray()\ntfidf_search = tfidf_vect.transform(features.search_term).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom scipy.spatial import distance\nfeatures[\"tfidfCS_desc\"] = [distance.cosine(tfidf_search[i], tfidf_des[i]) for i in range(0, len(tfidf_des))]\nfeatures[\"tfidfCS_title\"] = [distance.cosine(tfidf_search[i], tfidf_title[i]) for i in range(0, len(tfidf_title))]\nfeatures[\"tfidfCS_brand\"] = [distance.cosine(tfidf_search[i], tfidf_brand[i]) for i in range(0, len(tfidf_brand))]\nfeatures[\"tfidfCS_CA\"] = [distance.cosine(tfidf_search[i], tfidf_CA[i]) for i in range(0, len(tfidf_CA))]\n                                                                                                \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODEL Selection","metadata":{}},{"cell_type":"markdown","source":"Drop the categorical columns from the datasets and have only numerical columns for regression","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfeatures = pd.DataFrame(features).fillna(0)\nmod_train = features.iloc[:74067]\nmod_test = features.iloc[74067:]\ny_train = mod_train['relevance']\nmod_train = mod_train.drop(columns=['product_title','product_description','brand','search_term','relevance','combined_attr'])\nmod_test = mod_test.drop(columns=['product_title','product_description','brand','search_term', 'relevance','combined_attr'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(mod_train), ' ',len(mod_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"METRIC\n","metadata":{}},{"cell_type":"markdown","source":"Define a function to evaluate the model based on MAE, MSE metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\ndef modelEvaluate(model, X_train, y_train, x_val, y_val, label_encode = None):\n    model.fit(X_train, y_train)\n    pred = model.predict(x_val)\n    _pred = pred\n    _y_val = y_val\n    if label_encode is not None:\n        _y_val = label_encode.inverse_transform(_y_val)\n        _pred = label_encode.inverse_transform(_pred)\n    mae, mse = (mean_absolute_error(_pred, _y_val),mean_squared_error(_pred, _y_val))\n    return (mae, mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport math \ndef crossValidate(model, label_encoder = None):\n    mae_score = []\n    mse_score = []\n    kf = KFold(n_splits=4)\n    kf.get_n_splits(mod_train)\n    for train_index, test_index in kf.split(mod_train):\n        X, X_test = mod_train.iloc[train_index], mod_train.iloc[test_index]\n        y, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n        (mae, mse) = modelEvaluate(model, X, y, X_test, y_test, label_encode = label_encoder)\n        mae_score.append(mae)\n        mse_score.append(mse)\n    return [sum(mae_score)/len(mae_score), math.sqrt(sum(mse_score)/len(mse_score))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"REGRESSION","metadata":{}},{"cell_type":"markdown","source":"Cross validation model to compare models of Random forest, XG Boost and Gradient Boosting with MAE, MSE scores","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn import linear_model\nimport xgboost\nfrom xgboost import XGBRegressor\nresult = []\nresult.append(('RandomForest',crossValidate(RandomForestRegressor(n_estimators=700, max_depth=6, random_state=42))))\nresult.append(('XGBoost',crossValidate(XGBRegressor(colsample_bytree=0.4,       \n                 learning_rate=0.1,\n                 max_depth=6,\n                 n_estimators=700,                                                                    \n                 reg_alpha=0.075,\n                 reg_lambda=0.045,\n                 subsample=0.55,\n                 seed=42))))\nresult.append(('GradientBoosting',crossValidate(GradientBoostingRegressor(n_estimators=700, max_depth=6, random_state=42))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Building of Stacked regressor combining the estimators of random forest, xgboost and taking final estimator from BayesianRidge","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import linear_model\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import BayesianRidge\nestimators = [  ('RandomForest', RandomForestRegressor(n_estimators=700, max_depth=6, random_state=42)),\n                ('XGBoost', XGBRegressor(colsample_bytree=0.4,learning_rate=0.1,max_depth=6,n_estimators=700,reg_alpha=0.075,reg_lambda=0.045,\n                                         subsample=0.55,seed=42)),                                                                    \n               \n             ]\nstacked_reg = StackingRegressor(\n        estimators=estimators,\n        final_estimator=BayesianRidge()\n    )\nresult.append(('StackedBayesianRidge',crossValidate(stacked_reg)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the MAE,RMSE of the 4 models - RF, XGBoost, Gradient, StackedBayesian","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nlabels = []\nmae = []\nrmse = []\nfor _name, _score in result:\n    labels.append(_name)\n    mae.append(_score[0])\n    rmse.append(_score[1])\n\n#Plotting\nx = np.arange(len(labels))  # the label locations\nwidth = 0.3  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(15,6))\nrects1 = ax.bar(x - width/2, mae, width, label='MAE')\nrects2 = ax.bar(x + width/2, rmse, width, label='RMSE')\n\nax.set_ylabel('Scores')\nax.set_title('Scores by models')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fitting the model on train dataset and validating the model for prediction of relevance scores in test dataset","metadata":{}},{"cell_type":"code","source":"stacked_reg.fit(mod_train, y_train)\npred = stacked_reg.predict(mod_test)\npred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determine the importance of features in the model and arranging them in the order of highest importance","metadata":{}},{"cell_type":"code","source":"fi = pd.DataFrame({'importance': stacked_reg.feature_names_in_}).sort_values(by='importance', ascending=False)\n#fi = fi.reset_index()\nfi.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate the submission file with the relevance scores that are predicted from the stacked regressor model","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/sample/sample_submission.csv')\nsubmission['relevance'] = pred\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}